{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Famous Pre-trained Image Models\n",
    "# VGG16 (visual geometry group, by Oxford)\n",
    "# VGG19\n",
    "# InceptionV3 (by Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Googles Goggles is the beginning of visual search technology.\n",
    "# With this image recognition app, users can take a photo of a physical object, and Google will try to find information about what is pictured.\n",
    "\n",
    "# Take a photo of a landmark and Google Goggles can give you its history.\n",
    "# Snap a pic of a foreign menu, and it can be translated. \n",
    "# the app can recognise and generate informaation on books, CDs, virtually anything that is 2D.\n",
    "\n",
    "# business value:\n",
    "# another avenue to generate search data\n",
    "# recommend users to advertisers and retailers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/vgg16_croped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "University of Oxford Visual Geometry Group has developed VGG-16 trained weights [Here](https://github.com/fchollet/deep-learning-models/releases)\n",
    "\n",
    "Download the [tensorflow h5 file](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5). \n",
    "\n",
    "Note this file is a little over half a gigabyte, so it will take a while to download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "weight_file = 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    raise FileNotFoundError(\"No file {weight_file} found. Check path again\".format(weight_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_file = 'vgg19_weights_tf_dim_ordering_tf_kernels.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 31675  100 31675    0     0   8892      0  0:00:03  0:00:03 --:--:--  8892\n"
     ]
    }
   ],
   "source": [
    "# Download labels for VGG16\n",
    "!curl https://raw.githubusercontent.com/torch/tutorials/master/7_imagenet_classification/synset_words.txt -o synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Convolutional Neural Network Architecture\n",
    "\n",
    "### VGG (Loading a Pretrained Network using keras utilities)\n",
    "\n",
    "this network has been pretrained on a large dataset (imagenet) as the basis for an image classifier. It has taken a huge amount of gpu time/power and data to train this model.\n",
    "\n",
    "Here are [more examples of keras transfer learning](https://keras.io/applications/) with modern pretrained CNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.common.set_image_dim_ordering('th')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset = pd.read_csv('synset_words.txt', skipinitialspace=True, names = ['synset', 'words'])\n",
    "\n",
    "# model = VGG16(weights='imagenet')\n",
    "model = VGG_16(weight_file)   # note that we don't actually train/adjust the weights at all here\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_bgr_numpy_array(image_path, size=(224,224)):\n",
    "    \"\"\"The network has been trained using opencv and BGR images \n",
    "    (i.e. channels order blue, green, red rather than red, green, blue).\n",
    "    The description of why is https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb\n",
    "    \n",
    "    We can use a simpler image library as long as we manually convert\n",
    "    the data to the expected format.\n",
    "    \"\"\"\n",
    "    image = PIL.Image.open(image_path).resize(size)\n",
    "    img_data = np.array(image.getdata(), np.float32).reshape(*size, -1)\n",
    "    # swap R and B channels\n",
    "    img_data = np.flip(img_data, axis=2)\n",
    "    return img_data\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    im = convert_image_to_bgr_numpy_array(image_path)\n",
    "\n",
    "    # these subtractions are just mean centering the images \n",
    "    # based on known means for different color channels\n",
    "    im[:,:,0] -= 103.939\n",
    "    im[:,:,1] -= 116.779\n",
    "    im[:,:,2] -= 123.68\n",
    "\n",
    "    im = im.transpose((2,0,1)) # adjust from (224, 224, 3) to (3, 224, 224) for keras\n",
    "    im = np.expand_dims(im, axis=0) # adjust to (1, 3, 224, 224) for generating keras prediction\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InceptionV3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-4d663539c765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img/dog_2.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InceptionV3' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "img = prepare_image('img/dog_2.jpg')\n",
    "\n",
    "model = InceptionV3(weights='imagenet')\n",
    "out = model.predict(img)\n",
    "y_pred = np.argmax(out)\n",
    "\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGG19' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-95af906309a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img/test.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VGG19' is not defined"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/test.jpg')\n",
    "\n",
    "model = VGG19(weights='imagenet')\n",
    "out = model.predict(img)\n",
    "y_pred = np.argmax(out)\n",
    "\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07930864', 'cup', 0.6994144), ('n03063599', 'coffee_mug', 0.18904433), ('n04131690', 'saltshaker', 0.020779125), ('n03063689', 'coffeepot', 0.011247833), ('n04423845', 'thimble', 0.0071213855)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/sloth.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02089973', 'English_foxhound', 0.87461126), ('n02089867', 'Walker_hound', 0.121277235), ('n02088364', 'beagle', 0.0036947893), ('n02088238', 'basset', 0.00016243898), ('n02088466', 'bloodhound', 0.00014211661)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/beagle.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02109047', 'Great_Dane', 0.55036515), ('n02099712', 'Labrador_retriever', 0.27059543), ('n02087394', 'Rhodesian_ridgeback', 0.050941028), ('n02090379', 'redbone', 0.019947253), ('n02089973', 'English_foxhound', 0.019448861)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Labrador.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02113799', 'standard_poodle', 0.5172874), ('n02113712', 'miniature_poodle', 0.39728504), ('n02113624', 'toy_poodle', 0.083053865), ('n02088094', 'Afghan_hound', 0.00066289917), ('n02105505', 'komondor', 0.00046583617)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Poodle.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02085936', 'Maltese_dog', 0.27990836), ('n02098286', 'West_Highland_white_terrier', 0.14876175), ('n02113624', 'toy_poodle', 0.12125379), ('n02113712', 'miniature_poodle', 0.10880081), ('n02113978', 'Mexican_hairless', 0.108365685)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Dog_3.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02108915', 'French_bulldog', 0.752249), ('n02096585', 'Boston_bull', 0.15032904), ('n02085620', 'Chihuahua', 0.059806228), ('n02087046', 'toy_terrier', 0.012236713), ('n02112706', 'Brabancon_griffon', 0.002886072)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Chihuahua.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02109525', 'Saint_Bernard', 0.3003843), ('n02108915', 'French_bulldog', 0.27818522), ('n02093428', 'American_Staffordshire_terrier', 0.058999773), ('n02108089', 'boxer', 0.048719466), ('n02110958', 'pug', 0.03343548)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Bulldog.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n03991062', 'pot', 0.7327521), ('n12620546', 'hip', 0.04760818), ('n12768682', 'buckeye', 0.029295594), ('n03457902', 'greenhouse', 0.024531063), ('n03930313', 'picket_fence', 0.017273879)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/test1.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07614500', 'ice_cream', 0.28780892), ('n07579787', 'plate', 0.16033651), ('n04476259', 'tray', 0.042177286), ('n07836838', 'chocolate_sauce', 0.031268515), ('n12144580', 'corn', 0.025088983)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/test2.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07745940', 'strawberry', 0.34342515), ('n07742313', 'Granny_Smith', 0.14368135), ('n12768682', 'buckeye', 0.077075586), ('n07753592', 'banana', 0.064830475), ('n07753113', 'fig', 0.056029476)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/mangosteen.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07745940', 'strawberry', 0.99977857), ('n04332243', 'strainer', 3.446263e-05), ('n07747607', 'orange', 2.406561e-05), ('n07753592', 'banana', 2.256647e-05), ('n07768694', 'pomegranate', 2.1113809e-05)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/strawberry.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07614500', 'ice_cream', 0.20130746), ('n03476684', 'hair_slide', 0.057905596), ('n07579787', 'plate', 0.057216298), ('n07745940', 'strawberry', 0.054555055), ('n07714571', 'head_cabbage', 0.050523743)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/icecream.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07614500', 'ice_cream', 0.87625885), ('n07613480', 'trifle', 0.11704696), ('n07836838', 'chocolate_sauce', 0.004256764), ('n07745940', 'strawberry', 0.0008514527), ('n07579787', 'plate', 0.0003462588)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/icecream2.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer Learning\n",
    "\n",
    "it turns out that the lower level featured learned by VGG16 on imagenet are still applicable to other problems with natural images. If we can preserve the lower-level features, we can just train a new model on those features. (In fact, in the case of 'softmax', we can think of this as just training a new multinomial logistic regression, on those convolution features)\n",
    "\n",
    "Lets just snip off last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Caveat\n",
    "\n",
    "if we just add a new layer with default weights, it is going to be very wrong the first iteration. Since it is so wrong, the gradient will be huge, and because we are using back propagation those errors will be sent down stream into the lower level features. This can quickly destroy the rest of the network.\n",
    "\n",
    "In order to retrain this model we must protect the lower-level features, until our new layers have reached more stability. We can do this by freezing those layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we'll add our new layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# note we exclude the final dense layers and add one back below, we would retrain it ourselves\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(3,224,224)) \n",
    " \n",
    "# Freeze convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False    \n",
    "    \n",
    "x = base_model.output\n",
    "x = Flatten()(x) # flatten from convolution tensor output \n",
    "predictions = Dense(2, activation='softmax')(x) # should match # of classes predicted\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block5_pool/transpose_1:0' shape=(?, 512, 7, 7) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flatten_2/Reshape:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "            loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then you would just train like normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "# i.e. if we had training images and our own labels, we could run\n",
    "model.fit(X_train,y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How much data do you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Actually with this bottleneck approach, you don't need as much. 200-1000 representitive images of each class will give good results. Because\n",
    "* Google has already done most of the hard work\n",
    "* We can use image augmentation to increase our number of training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "New Architectures are being published every day. So much to read!\n",
    "\n",
    "* [Curated List of Deep Learning papers](https://github.com/ChristosChristofidis/awesome-deep-learning)\n",
    "* [Good reddit post for keeping up with the latest research](https://www.reddit.com/r/MachineLearning/comments/6d7nb1/d_machine_learning_wayr_what_are_you_reading_week/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
